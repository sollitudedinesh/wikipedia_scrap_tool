# Wikipedia Data Extraction and Query Tool

This project extracts data from a Wikipedia page, loads it into a vector database, and uses a generative AI model to answer questions based on the loaded data. The application is implemented using Python and FastAPI.

## Table of Contents

- [Features](#features)
- [Technologies Used](#technologies-used)
- [Endpoints](#endpoints)
- [Setup and Installation](#setup-and-installation)
- [How to Run](#how-to-run)
- [Usage](#usage)
- [Best Practices Followed](#best-practices-followed)
- [Error Handling and Validation](#error-handling-and-validation)

## Features

1. **Data Extraction**: Extracts content from a specified Wikipedia page.
2. **Vector Database**: Embeds the extracted data using a pre-trained model and stores it in a vector database (Milvus or in-memory).
3. **Generative AI Query**: Answers user queries based on the extracted data using a generative AI model.
4. **FastAPI Endpoints**: Two endpoints: one for loading data and another for querying the AI model.

## Technologies Used

- **Programming Language**: Python
- **Web Scraping**: BeautifulSoup
- **Vector Database**: Milvus (or in-memory as an alternative)
- **Embedding Model**: Sentence Transformers
- **Generative AI Model**: OpenAI GPT or Hugging Face Transformers
- **API Framework**: FastAPI

## Endpoints

### 1. `/load` (POST)
- **Description**: Extracts data from a Wikipedia page, preprocesses it, and loads it into the vector database.
- **Input**: JSON body with a key `url`, which contains the Wikipedia page URL.
- **Output**: Success message if data is successfully loaded into the vector database.

**Example Request**:
```json
{
  "url": "https://en.wikipedia.org/wiki/Natural_language_processing"
}
```
### 2. `/query` (POST)
- **Description**: Queries the vector database and generates an answer using the generative AI model.
- **Input**: JSON body with a key query, which contains the user query.
- **Output**: The answer generated by the AI model.
**Example Request**:
```json
   {
  "query": "What is natural language processing?"
}
   ```

## Setup and Installation
# Prerequisites
Ensure you have the following installed:
- Python 3.8+
- pip (Python package manager)
  
### Step 1: Clone the Repository

```bash
git clone https://github.com/your-username/wikipedia_scrap_tool.git
cd wikipedia_scrap_tool
```
### Step 2: Install Required Dependencies
```bash
pip install -r requirements.txt
```
### Step 3: Set Up Environment Variables
Create a .env file in the root directory and add the following:

```bash
OPENAI_API_KEY=your_openai_api_key
```
### Step 4: Start Milvus (if using Milvus)
Follow the official Milvus installation guide to set up and start the Milvus server.

### Step 5: Run the FastAPI Application
```bash
uvicorn app.main:app --reload
```
The application will run at http://127.0.0.1:8000.

## How to Run
### Load Data:

Use the /load endpoint to extract and load data from a Wikipedia page.
**Example:**
```bash
curl -X 'POST' \
'http://127.0.0.1:8000/load' \
-H 'Content-Type: application/json' \
-d '{"url": "https://en.wikipedia.org/wiki/Artificial_intelligence"}'

```
## Query Data:

Use the /query endpoint to ask questions based on the loaded data.
### Example:
```bash
curl -X 'POST' \
'http://127.0.0.1:8000/query' \
-H 'Content-Type: application/json' \
-d '{"query": "What is artificial intelligence?"}'
```
## Best Practices Followed
- Code Readability: The code follows PEP8 standards, with proper variable names and inline comments.
- Separation of Concerns: The code is modular, with separate functions for data extraction, preprocessing, embedding, and querying.
- Environment Variables: Sensitive data like API keys are stored in .env files and not hardcoded.
- Logging: Logs are implemented to track the success or failure of data loading and querying operations.
- Documentation: Every function is well-documented using docstrings.
## Error Handling and Validation
- Input Validation: The /load and /query endpoints validate that the necessary inputs (e.g., url and query) are provided and formatted correctly.
- Error Handling:
  1. If the Wikipedia page cannot be scraped, an error message is returned.
  2. If the vector database is not accessible, the user is notified.
  3. If the query does not match any content, an appropriate response is given.
